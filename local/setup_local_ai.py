#!/usr/bin/env python3
"""
Automated setup for local AI with Ollama
"""

import subprocess
import requests
import time
import json
import sys

def wait_for_ollama(max_retries=30):
    """Wait for Ollama to be ready"""
    print("Waiting for Ollama to start...")
    for i in range(max_retries):
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=2)
            if response.status_code == 200:
                print("‚úÖ Ollama is ready")
                return True
        except:
            pass
        time.sleep(2)
        print(f"  Attempt {i+1}/{max_retries}...")

    print("‚ùå Ollama failed to start")
    return False

def pull_model(model_name):
    """Pull a model using Ollama API"""
    print(f"Pulling {model_name}...")

    try:
        response = requests.post(
            "http://localhost:11434/api/pull",
            json={"name": model_name},
            stream=True,
            timeout=300
        )

        if response.status_code == 200:
            for line in response.iter_lines():
                if line:
                    try:
                        data = json.loads(line)
                        if "status" in data:
                            print(f"  {data['status']}")
                        if data.get("status") == "success":
                            print(f"‚úÖ {model_name} ready")
                            return True
                    except:
                        pass

        print(f"‚ùå Failed to pull {model_name}")
        return False

    except Exception as e:
        print(f"‚ùå Error pulling {model_name}: {e}")
        return False

def test_model(model_name):
    """Test the model with a simple prompt"""
    print(f"Testing {model_name}...")

    try:
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": model_name,
                "prompt": "Explain what Terraform is in one sentence.",
                "stream": False,
                "options": {"num_predict": 50}
            },
            timeout=30
        )

        if response.status_code == 200:
            result = response.json()
            answer = result.get("response", "").strip()
            if answer:
                print(f"‚úÖ {model_name} test successful")
                print(f"   Response: {answer[:100]}...")
                return True

        print(f"‚ùå {model_name} test failed")
        return False

    except Exception as e:
        print(f"‚ùå Error testing {model_name}: {e}")
        return False

def setup_local_ai():
    """Setup local AI with best available model"""

    print("üöÄ Setting up CloudOps Assistant Local AI")
    print("=" * 50)

    # Wait for Ollama to be ready
    if not wait_for_ollama():
        print("\n‚ùå Setup failed: Ollama not available")
        print("Make sure Docker Compose is running: docker-compose up -d")
        return False

    # Try models in order of preference (smallest to largest)
    models = [
        "gemma3:270m"
    ]

    for model in models:
        print(f"\nüì¶ Trying {model}...")
        if pull_model(model):
            if test_model(model):
                print(f"\nüéâ Successfully set up {model}")
                print(f"CloudOps Assistant will use {model} for AI features")

                # Update the AI provider configuration
                update_ai_config(model)
                return True
            else:
                print(f"Model {model} pulled but failed test")
        else:
            print(f"Failed to pull {model}")

    print("\n‚ùå No models could be set up")
    return False

def update_ai_config(model_name):
    """Update AI configuration to use the selected model"""
    config = f"""# Local AI Configuration
# Generated by setup_local_ai.py

OLLAMA_MODEL={model_name}
OLLAMA_URL=http://localhost:11434
AI_PROVIDER=ollama
"""

    try:
        with open("ai_config.env", "w") as f:
            f.write(config)
        print(f"‚úÖ AI configuration saved to ai_config.env")
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not save config: {e}")

if __name__ == "__main__":
    success = setup_local_ai()
    sys.exit(0 if success else 1)
